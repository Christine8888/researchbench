import os
import json
import yaml
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Union
from openai import OpenAI, AzureOpenAI
from anthropic import Anthropic
from together import Client


class LLMClient(ABC):
    """base class for language model clients"""
    def __init__(
        self, 
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        system_prompt: str = "",
        **kwargs
    ):
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt
        self.extra_params = kwargs
        self.client = None
    
    @abstractmethod
    def initialize_client(self, **kwargs):
        """initialize specific provider client"""
        pass
    
    @abstractmethod
    def generate_text(self, prompt: str) -> str:
        """generate text using the API"""
        pass
    
    def set_system_prompt(self, system_prompt: str):
        """Update the system prompt"""
        self.system_prompt = system_prompt


class OpenAIClient(LLMClient):
    """OpenAI API client implementation"""
    
    def __init__(
        self, 
        api_key: str, 
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        system_prompt: str = "",
        **kwargs
    ):
        super().__init__(model_name, temperature, max_tokens, system_prompt, **kwargs)
        self.api_key = api_key
        self.initialize_client()
    
    def initialize_client(self, **kwargs):
        self.client = OpenAI(api_key=self.api_key)
    
    def generate_text(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            **self.extra_params
        )
        return response.choices[0].message.content


class AzureOpenAIClient(LLMClient):
    """Azure OpenAI API client implementation"""
    
    def __init__(
        self, 
        api_key: str,
        endpoint: str,
        api_version: str,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        system_prompt: str = "",
        **kwargs
    ):
        super().__init__(model_name, temperature, max_tokens, system_prompt, **kwargs)
        self.api_key = api_key
        self.endpoint = endpoint
        self.api_version = api_version
        self.initialize_client()
    
    def initialize_client(self, **kwargs):
        self.client = AzureOpenAI(
            api_key=self.api_key,
            azure_endpoint=self.endpoint,
            api_version=self.api_version
        )
    
    def generate_text(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            **self.extra_params
        )
        return response.choices[0].message.content


class AnthropicClient(LLMClient):
    """Anthropic API client implementation"""
    
    def __init__(
        self, 
        api_key: str,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        system_prompt: str = "",
        **kwargs
    ):
        super().__init__(model_name, temperature, max_tokens, system_prompt, **kwargs)
        self.api_key = api_key
        self.initialize_client()
    
    def initialize_client(self, **kwargs):
        self.client = Anthropic(api_key=self.api_key)
    
    def generate_text(self, prompt: str) -> str:
        response = self.client.messages.create(
            model=self.model_name,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            system=self.system_prompt,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }
            ],
            **self.extra_params
        )
        return response.content[0].text


class TogetherClient(LLMClient):
    """Together API client implementation"""
    
    def __init__(
        self, 
        api_key: str,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        system_prompt: str = "",
        **kwargs
    ):
        super().__init__(model_name, temperature, max_tokens, system_prompt, **kwargs)
        self.api_key = api_key
        self.initialize_client()
    
    def initialize_client(self, **kwargs):
        self.client = together.Client(api_key=self.api_key)
    
    def generate_text(self, prompt: str) -> str:
        result = self.client.Completion.create(
            model=self.model_name,
            prompt=f"{self.system_prompt}\n\n{prompt}",
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            **self.extra_params
        )
        return result.content


class ClientFactory:
    """create LLM clients from provider"""
    
    @staticmethod
    def create_client(provider: str, **config) -> LLMClient:
        """
        create and return the appropriate LLM client based on provider
        
        Args:
            provider: LLM provider name (openai, azure, anthropic, together)
            **config: configuration parameters for the client
            
        Returns:
            initialized LLM client
        """
        provider = provider.lower()
        
        if provider == "openai":
            return OpenAIClient(**config)
        elif provider == "azure":
            return AzureOpenAIClient(**config)
        elif provider == "anthropic":
            return AnthropicClient(**config)
        elif provider == "together":
            return TogetherClient(**config)
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    @staticmethod
    def from_config(config_path: str) -> LLMClient:
        """
        create an LLM client from a configuration file
        
        Args:
            config_path: path to the YAML/JSON configuration file
            
        Returns:
            initialized LLM client
        """
        if not os.path.isfile(config_path):
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        # load configuration
        if config_path.endswith(".yaml") or config_path.endswith(".yml"):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        else:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
        
        # extract provider and create client
        provider = config.pop("provider", None)
        if not provider:
            raise ValueError("Provider not specified in config file")
        
        return ClientFactory.create_client(provider, **config)